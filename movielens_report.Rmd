---
title: "MovieLens"
output: pdf_document
header-includes:
   - \usepackage{float}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, fig.height = 3, fig.width = 3, fig.align = "center", fig.pos = "H")
```

```{r dataset_retrieval, include = FALSE}
################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

```{r main_functions, echo = FALSE}
#Returns the average ratings for the given training set.
get_aver_ratings <- function(train_set) {
  mean(train_set$rating)
}

#Calculates the bias associated with the movies, one of the constituent elements of the final predicting model.
get_bias_movies <- function(train_set, aver_r, lambda) {
  train_set %>% group_by(movieId) %>% 
				        summarize(bias_m = sum(rating - aver_r) / (n() + lambda))
}

#Calculates the bias associated with the users, one of the constituent elements of the final predicting model.
get_bias_users <- function(train_set, aver_r, bias_m, lambda) {
  train_set %>% left_join(bias_m, by="movieId") %>% 
				        group_by(userId) %>% 
				        summarize(bias_u = sum(rating - bias_m - aver_r) / (n() + lambda)) 
}

#It returns the predictions on the test set by applying the given lambda and the training set information. 
get_prediction <- function(train_set, test_set, lambda, cleaning = TRUE, only_pred = TRUE) {
  
  #Getting the essential parts of the predictive model by using the training set.
  aver_r <- get_aver_ratings(train_set)
  bias_m <- get_bias_movies(train_set, aver_r, lambda)
  bias_u <- get_bias_users(train_set, aver_r, bias_m, lambda)
  
  #Applying the values generated by using the training set to make the predictions on the test set. 
  output <- test_set %>% 
			      left_join(bias_m, by = "movieId") %>% 
			      left_join(bias_u, by = "userId") %>% 
			      mutate(pred = aver_r + bias_m + bias_u)
  
  if (cleaning) {
    #Replacing invalid values caused by wrong input data and calculations. 
    min_r <- min(train_set$rating)
    max_r <- max(train_set$rating)
    output <- output %>% mutate(
              pred = ifelse(is.na(pred), aver_r, #NA to average.
              ifelse(pred < min_r, min_r, #Below minimum to minimum. 
              ifelse(pred > max_r, max_r, #Above maximum to maximum. 
              pred))))
  }
  
  if (only_pred) {
    #Returning only the predictions.
    output$pred 
  }
  else { 
    #Returning all the variables (inputs, predictions and intermediate calculations).
    output 
  }
}

#Finds out what is the best value of lambda for the given data set.
get_lambda <- function(train_set) {
  
  #Dividing the training data set into training/test subsets to emulate more realistic conditions.
  test_subindex <- createDataPartition(train_set$movieId, times = 1, p = 0.5, list = FALSE)
  train_subset <- train_set %>% slice(-test_subindex)
  test_subset <- train_set %>% slice(test_subindex)
  
  #In order to know what is the best lambda value under the current conditions, the input information
  #is used to get predictions by trying a reasonably big number of potential lambda values.
  lambdas <- seq(0, 7, 0.5) 
  rmses <- sapply(lambdas, function(lambda) { 
                  pred <- get_prediction(train_subset, test_subset, lambda)
			            return(RMSE(pred, test_subset$rating))})
  
  #The lambda value delivering the lowest RMSE is assumed to be the best one.
  lambdas[which.min(rmses)] 
}

#Returns the RMSE associated with the input values.
get_rmse <- function(pred, real) { 
  sqrt(mean((pred - real)^2)) 
}

#Defining the training and test sets.
train_set <- edx 
test_set <- validation
```

```{r significant_digits, echo = FALSE}
digits <- 5
```

```{r lambda, echo = FALSE}
lambda <- get_lambda(train_set)
```

```{r final_prediction, echo = FALSE}
prediction <- get_prediction(train_set, test_set, lambda, TRUE, FALSE)
```

```{r final_rmse, echo = FALSE}
rmse <- round(get_rmse(prediction$pred, test_set$rating), digits)
```

# Introduction

This document describes the main steps taken to develop a model predicting the variable ```rating``` in the test set ```validation```, after being trained with the information contained in the training set ```edx```. Both sets represent a small sample from a comprehensive data source about MovieLens, an online movie recommendation system.

Out of the five predictors included in the input datasets, the current model is only considering ```userId``` (integer) and ```movieId``` (integer). Some of the remaining variables might contain relevant information and their analysis is a matter of future work. 

The model is based on the following main ideas:

- Under these conditions, averages provide good preliminary estimates. 
- The data is irregularly distributed (e.g., some movies get many more ratings than others).
- The predictions are the result of adding the effects at three different levels: all the movies, each individual movie and each individual user.

The accuracy of the generated predictions is determined using the root mean square error (RMSE). The obtained value (`r rmse`) indicates a good performance.

# Analysis {#Analysis}

Firstly, it is important to adequately understand the underlying reality with a major focus on the variable to be predicted. It can be summarised in these points:

- Subjective assessments tend to be somehow conditioned by the given environment. That is, the ratings issued by the users in MovieLens are likely to meet certain common criteria.
- A significant proportion of movies tends to be liked or disliked by most of people. Scenarios where opinions are distributed approximately equally can be considered almost exceptional.
- Usually, somewhat homogeneous groups of people like and dislike different types of movies.

The following graphs show how ratings are distributed across a group of fifty random users and movies. It is clear that some movies are rated more often than others, and also that some users are more active contributors. Both conclusions are fully compatible with what is expected, but the effects from these irregular distributions of ratings need to be somehow addressed.   

```{r distribution_histograms, fig.show = "hold", out.width = "50%", fig.cap = "Rating frequencies.", echo = FALSE}
ids <- sample.int(1000, 50) #50 integers randomly selected from 1 to 1000. 

data <- train_set %>% group_by(movieId) %>% filter(movieId %in% ids) %>% summarize(count = n())
hist(data$count, main="Movies", xlab="Number of ratings")

data <- train_set %>% group_by(userId) %>% filter(userId %in% ids) %>% summarize(count = n())
hist(data$count, main="Users", xlab="Number of ratings")
```

By applying the previous ideas, a preliminary modelling approach can be defined with the following equation:

\begin{center}
$rating_{u,m}=aver_r+\epsilon_{u,m}$
\end{center}

It outputs the rating given by each user to each movie, with $\epsilon_{u,m}$ representing the independent errors and $aver_r$ the average value of all the ratings across the whole data set. Note that this last variable is already accounting for the aforementioned environmental effects via assuming that all the ratings are somehow related.

As expected, the accuracy of such a simplistic model is not particularly good. More specifically, the proposed loss function (i.e., RMSE) delivers a modest `r round(get_rmse(get_aver_ratings(train_set), test_set$rating), digits)`. In any case, it does already represent an acceptable starting point. Additionally, its average-based essence also seems the best way to go further.

Some movies tend to be liked more than others and this fact has to be taken into account when estimating user ratings. This preliminary model can, consequently, be improved by bringing all those movie-specific effects into consideration. Mathematically, it can be represented as the average difference between the ratings for the given movie and the default estimates (i.e., predictions generated by the model as defined up to this point). The term "bias" adequately describes this and subsequent additions. The new model is hence defined by:

\begin{center}
$rating_{u,m}=aver_r+bias_m+\epsilon_{u,m}$
\end{center}

There is still a relevant aspect which has been ignored so far. Users also have their own preferences and tend to like some movies more than what most people do. This can again be modelled as the average of the variations between the predictions so far and the ratings issued by the given user to all the movies. After this last inclusion, the model becomes:

\begin{center}
$rating_{u,m}=aver_r+bias_m+bias_u+\epsilon_{u,m}$
\end{center}

The previous extensions have improved the accuracy of the model until getting a RMSE value of `r round(get_rmse(get_prediction(train_set, test_set, 0, FALSE, TRUE), test_set$rating), digits)`. This is appreciably better, but it can still be improved further.

The uneven distribution of ratings across movies and users is likely to have an appreciable impact on the reliability of the model. Regularisation seems the best way around this problem: predictions generated from small sample sizes will be penalised. Mathematically, the model is updated by redefining the way in which averages are being calculated via adding a constant (```lambda```) to the corresponding total number of elements in the denominators. 

The best value for the aforementioned ```lambda``` parameter is intrinsically associated with the given data and its determination is not immediate. Logically, only the information contained in the training data set can be used to find that value out. On the other hand, it is definitely possible to perform some modifications in order to better resemble the actual testing conditions. More specifically, the training data set can be divided into two subsets (training and testing) to ensure as realistic conditions as possible. That is, various candidates (i.e., the model up to this point including different values for the new ```lambda``` constant) are trained on and the accuracy of their predictions compared against the corresponding data subsets. The best ```lambda``` (`r lambda`) is assumed to be the one from the candidate model with the smallest RMSE. 

As shown in the table below, the top and bottom predictions are outside the training data boundaries (i.e., `r max(train_set$rating)` and `r min(train_set$rating)`). These outliers surely have a negative impact on the RMSE value. The correction is trivial: replacing these outliers with the aforementioned boundaries.

```{r top_bottom, echo = FALSE}
preds <- round(get_prediction(train_set, test_set, lambda, FALSE, TRUE), digits) 

knitr::kable(data.frame(
            Top = sort(preds, decreasing = TRUE)[1:10], Bottom = sort(preds)[1:10]),
            caption = "Top and bottom predictions.", align = "c")
```

# Results {#Results}

By bearing in mind that, under the current conditions, a RMSE value below 0.86490 can be considered a success, the performance of the final model (`r rmse`) is undoubtedly good.

```{r correlation_movies, echo = FALSE}
corr_movies <- prediction %>% 
              group_by(movieId) %>% 
              summarize(movies_prediction = mean(pred), movies_original = mean(rating))
```

The model predictions are showing a strong correlation (`r round(cor(corr_movies$movies_prediction, corr_movies$movies_original), digits)`) with respect to the original ratings when considering averages across all the movies.

```{r correlation_movies_graph, fig.cap = "Correlation of movies.", echo = FALSE}
corr_movies %>% ggplot() + 
                geom_point(aes(x = movies_original, y = movies_prediction)) + 
                xlab("Real") + ylab("Predictions")
```

```{r correlation_users, echo = FALSE}
corr_users <- prediction %>% 
              group_by(userId) %>% 
              summarize(users_prediction = mean(pred), users_original = mean(rating))
```

When grouping by users, the correlation between the predictions and the original values is a bit weaker (`r round(cor(corr_users$users_prediction, corr_users$users_original), digits)`), but still good enough, as shown in the graph below.

```{r correlation_users_graph, fig.cap = "Correlation of users.", echo = FALSE}
corr_users %>% ggplot() + 
              geom_point(aes(x = users_original, y = users_prediction)) + 
              xlab("Real") + ylab("Predictions")
```

Additionally, the generated predictions will always lie within the training set boundaries and, consequently, no unrealistic ratings are expected.  

# Conclusion {#Conclusion}

The created model is undoubtedly fulfilling the main goals and expectations which motivated this work. Its accuracy is appreciably better than the most optimistic target. The delivered predictions are realistic and closely resemble the actual values under a wide variety of different scenarios. This model can definitively be assumed to somehow understand the reality underlying the input data sets.

The previous paragraph should not hide the fact that some improvements are certainly possible. At the user level, for example, the performance of the model is a bit lacking. It does not seem to account for the user-to-user variability as well as it could. 

The current model is not even considering some variables which might also contribute to providing a better picture. More specifically, analysing the effect of movie genres, especially on how users rate movies, is a matter of future work.

In summary, the model described in this paper can be considered a solid first step to face the numerical understanding of the data under consideration and, by extension, of movie ratings from Movielens or from any other source. Further work is certainly expected, but the main ideas exposed here are likely to remain essentially unaltered.